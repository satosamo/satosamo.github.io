Directory Tree:
satosamo.github.io/
│   ├── eleventy.config.js
│   ├── full_code.txt
│   ├── index.html
│   ├── readme.md
│   ├── .git/ [EXCLUDED]
│   ├── .github/ [EXCLUDED]
│   ├── .vscode/
│   │   ├── settings.json
│   ├── articles/
│   │   ├── 3dgs/
│   │   │   ├── intro.md
│   │   │   ├── original_paper.md
│   │   ├── compbio/
│   │   │   ├── course/
│   │   │   │   ├── cigar_strings.md
│   │   │   │   ├── intro.md
│   │   │   ├── journal/
│   │   │   │   ├── estimation_maximization.md
│   │   │   │   ├── intro_emu.md
│   │   │   │   ├── pipeline.md
│   │   │   │   ├── qna.md
│   │   ├── dipl/
│   │   │   ├── 0_abstract.md
│   │   │   ├── 0_details.md
│   │   │   ├── 0_introduction.md
│   │   │   ├── 0_literature.md
│   │   │   ├── 0_motivation.md
│   │   │   ├── 0_roadmap.md
│   │   │   ├── 0_thoughts.md
│   │   │   ├── 1_medical_scans.md
│   │   │   ├── 1_volume_reconstruction.md
│   │   ├── dlcv/
│   │   │   ├── homework1.md
│   │   ├── ds/
│   │   │   ├── cache.md
│   │   │   ├── flat_hash_map.md
│   │   │   ├── hashing.md
│   │   │   ├── hash_functions.md
│   │   │   ├── hash_tables.md
│   │   ├── mathematics/
│   │   │   ├── probstat/
│   │   │   │   ├── conditional_probability.md
│   │   │   │   ├── foundations.md
│   ├── css/
│   │   ├── banner.css
│   │   ├── katex.css
│   │   ├── markdown-containers.css
│   │   ├── page.css
│   │   ├── section.css
│   │   ├── sidebar.css
│   │   ├── widgets.css
│   ├── images/
│   │   ├── 1_1.svg
│   │   ├── construction.jpg
│   │   ├── roadmap.svg
│   │   ├── thoughts_1.svg
│   │   ├── uk_logo.ico
│   │   ├── uk_logo.jpg
│   │   ├── uk_logo.png
│   ├── node_modules/ [EXCLUDED]
│   ├── pages/
│   │   ├── supp_blog/
│   │   │   ├── p_0_radiancefields.md
│   ├── plugins/
│   │   ├── markdown-containers.js
│   ├── resources/
│   │   ├── papers/
│   │   │   ├── orig_cr_mc.pdf
│   ├── scripts/
│   │   ├── auth_hash.js
│   │   ├── auth_simple.js
│   │   ├── scrollarrow.js
│   │   ├── inertia_simulator/
│   │   │   ├── inertia_simulator.js
│   ├── sections/
│   │   ├── 3dgs.njk
│   │   ├── compbio.njk
│   │   ├── dipl.njk
│   │   ├── dlcv.njk
│   │   ├── ds.njk
│   │   ├── mathematics.njk
│   │   ├── supp_blog.njk
│   ├── unreleased/
│   │   ├── pointers_references.md
│   ├── _data/
│   │   ├── navigation.json
│   ├── _includes/
│   │   ├── base/
│   │   │   ├── base.njk
│   │   │   ├── base_article.njk
│   │   │   ├── base_section.njk
│   │   ├── partials/
│   │   │   ├── banner.njk
│   │   │   ├── homebutton.njk
│   │   │   ├── scrollarrow.njk
│   │   │   ├── sidebar.njk
│   │   ├── section_blocks/
│   │   │   ├── 3dgs/
│   │   │   │   ├── 3dgs_breakdown.njk
│   │   │   │   ├── blog.njk
│   │   │   ├── compbio/
│   │   │   │   ├── course.njk
│   │   │   │   ├── journal.njk
│   │   │   ├── mathematics/
│   │   │   │   ├── probstat.njk




# ======================
# File: eleventy.config.js
# ======================

// eleventy.config.js (ESM)

import markdownIt from "markdown-it";
import markdownItDeflist from "markdown-it-deflist";
import mk from "@vscode/markdown-it-katex";
import syntaxHighlight from "@11ty/eleventy-plugin-syntaxhighlight";

import configureContainers from "./plugins/markdown-containers.js";

export default function(eleventyConfig) {
  // Pass through static files
  eleventyConfig.addPassthroughCopy("index.html");
  eleventyConfig.addPassthroughCopy("css");
  eleventyConfig.addPassthroughCopy("images");
  eleventyConfig.addPassthroughCopy("resources");
  eleventyConfig.addPassthroughCopy("scripts");

  eleventyConfig.addPlugin(syntaxHighlight);

  const md = markdownIt({
    html: true,
    breaks: true,
    linkify: true,
  })
    .use(markdownItDeflist)
    .use(mk.default);

  configureContainers(md);

  eleventyConfig.setLibrary("md", md);

  // Process files from the 'articles' directory
  return {
    dir: {
      input: ".",
      output: "_site", // Final site build location
      includes: "_includes",
    },
  };
}

# ======================
# File: index.html
# ======================

<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Authorization</title>
    <link rel="icon" type="image/x-icon" href="/images/uk_logo.ico">
    <style>
        body {
            font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, sans-serif;
            display: grid;
            place-items: center;
            min-height: 80vh;
            background-color: #f4f4f4;
        }
        #auth-box {
            background: #fff;
            border: 1px solid #ddd;
            border-radius: 8px;
            padding: 2rem;
            box-shadow: 0 4px 12px rgba(0,0,0,0.05);
            text-align: center;
        }
        input {
            font-size: 1rem;
            padding: 0.5rem;
            margin-right: 0.5rem;
            border: 1px solid #ccc;
            border-radius: 4px;
        }
        button {
            font-size: 1rem;
            padding: 0.5rem 1rem;
            background-color: #007bff;
            color: white;
            border: none;
            border-radius: 4px;
            cursor: pointer;
        }
        button:hover {
            background-color: #0056b3;
        }
        #message {
            margin-top: 1rem;
            color: red;
            font-weight: bold;
        }
    </style>
</head>
<body>
    <div id="auth-box">
        <h2>Authorization</h2>
        <input type="password" id="passwordInput" placeholder="Password...">
        <button id="submitButton">Go</button>
        <p id="message"></p>
    </div>
    <script src = "/scripts/auth_simple.js"></script>
</body>
</html>


# ======================
# File: readme.md
# ======================

### For loops - blog lists

Every nunjucks tag ends with 's'. The for cycle in template uses:
- `tagname` for `tagnames`.
- example: `dlcv for dlcvs`

# ======================
# File: .vscode\settings.json
# ======================

{
    "chatgpt.commentCodeLensEnabled": false
}

# ======================
# File: articles\3dgs\intro.md
# ======================

---
title: "Introduction"
layout: "base/base_article.njk"
homeTag: "3dgs"
tags: "3dgs"
order : 1
---

# Introduction

TODO


# ======================
# File: articles\3dgs\original_paper.md
# ======================

---
title: "Breakdown of the Paper"
layout: "base/base_article.njk"
homeTag: "3dgs"
tags: "3dgs_breakdown"
order : 1
---

# Introduction

TODO
asddasasd

# ======================
# File: articles\compbio\course\cigar_strings.md
# ======================

---
title: "CIGAR strings"
layout: "base/base_article.njk"
homeTag: "compbio"
tags: "course_compbio"
order : 2
---

# CIGAR strings

The ‘CIGAR’ (Compact Idiosyncratic Gapped Alignment Report) string is how the SAM/BAM format represents spliced alignments.

CIGAR strings have a number of operators:

- $M$ - Match - Exact match of x positions
- $N$ - Alignment gap - Next x positions on ref don’t match
- $D$ - Deletion - Next x positions on ref don’t match
- $I$ - Insertion - Next x positions on query don’t match

**Example:**

```
CIGAR=3M2I3M:

0123456789
AAGTC  TAGAA (ref) 
  GTCGATAG (query)
```


# ======================
# File: articles\compbio\course\intro.md
# ======================

---
title: "Introduction"
layout: "base/base_article.njk"
homeTag: "compbio"
tags: "course_compbio"
order : 1
---

# Introduction

This blog accompanies the course, which covers fundamental concepts in molecular biology, algorithms, and machine learning relevant to computational biology. The course includes topics such as genome sequencing and assembly, gene finding, sequence alignment, evolutionary models, phylogenetic trees, comparative and population genomics, RNA structure, motif discovery, gene expression analysis, and protein structure and function. Selected current topics in the field are also included.

Students in computer science–oriented study programs will focus primarily on computational methods and mathematical modeling of these problems.

The purpose of this blog is to provide basic information, summaries, and supporting materials related to the topics discussed in the course.




# ======================
# File: articles\compbio\journal\estimation_maximization.md
# ======================

---
title: "Estimation-Maximization"
layout: "base/base_article.njk"
homeTag: "compbio"
tags: "compbio_blog_2"
order : 2
---

# Estimation-Maximization Algorithm

# ======================
# File: articles\compbio\journal\intro_emu.md
# ======================

---
title: "Introduction"
layout: "base/base_article.njk"
homeTag: "compbio"
tags: "compbio_intros"
order : 1
---

# Introduction to EMU

The paper we are studying presents a software tool called **Emu.** This tool was developed to generate accurate taxonomic abundance profiles from full-length 16s rRNA reads obtained by nanopore sequencing (ONT).

::: concept _Species
Taxon is any rank in the biological taxonomy system:
Domain -> Phylum -> Class -> Order -> Family -> Genus -> Species -> Lower
A species is the finest standard rank. In sequencing-based classification, it refers to organisms whose genomes differ enough that they can be considered distinct biological units. A genus-level classification is usually done by 16S short-read sequencing. A species-level classification is dependent on enough variable positions to tell apart species.
:::

Short reads coming from many different species at once are not assembled to contigs because it is not possible without creating chimeric, incorrect sequences. And since short reads only cover a small part of the 16S gene, they don’t observe enough species-specific differences and can't be used to reliably differentiate between them.

A long read, full-length 16S sequencing (the read cover the entire 16S gene) has the potential to allow species-level resolution. This is because full-length read aligns to the full-length reference which exposes the true mismatches at many distinct positions. However, full-length nanopore reads tend to have high error rates.

::: idea _Error rates
A nanopore measures ionic current as DNA passes through. Each 5-6-mer produces a characteristic current distribution. This is a indirect, hard inverse problem. Consequently, the signal for one base overlaps with the next because multiple consecutive bases are inside the pore simultaneously. At the same time, the motor protein does not pull DNA through at a perfectly constant rate. So the DNA can pause, jump or skip or accelerate and jitter because of thermal fluctuations.
:::

Emu solves this problem by using an **Expectation-Maximization algorithm** (EM) to distinguish between true biological differences and sequencing errors.


# ======================
# File: articles\compbio\journal\pipeline.md
# ======================

---
title: "Pipeline"
layout: "base/base_article.njk"
homeTag: "compbio"
tags: "compbio_blog_1"
order : 1
---

# Overview

Emu method distinguishes itself from other methods by not requiring consensus sequences but rather uses the information from the entire community (all the reads) to statistically correct abundance. This smooths-out sequencing noise. By re-estimating probabilities, Emu reduces false positives significantly compared to raw proper alignments.

::: concept Consensus Sequences
Multiple reads of the same molecule.
:::

Emu's algorithm is a two-stage process:

1. Classic proper alignments are generated between reads and the reference database
2. EM-based error-correction step is performed to iteratively refine species-level relative
abundances based on total read-mapping counts

### Notation

- $R$ - the set of reads
- we denote a single specific read as $r$
- $S$ - the set of references (the reference database)
- we denote a single specific sequence in the database as $s$
- $T$ - the set of all taxonomy identifications in $S$
- $F$ - **prior sample composition** the vector of probabilities (entries of the vector map one-to-one to species)
- the vector $F$ initially starts with uniformly distributed elements:
$$F(t)_{t\in T} = \frac{1}{|T|}$$
- $C$ - the set of all alignment types (e.g. mismatch, deletion)
- the alignment type event is denoted $c$
- $P(r | s)$ - the probability that (by nanopore sequencing) we get a read $r$ given we are sequencing a reference $s$, in other words the probability the a read $r$ aligns to reference sequence $s$
- $P(r | t)$ - the probability that (by nanopore sequencing) we get a read $r$ given the sequence comes from $t$
- $P(t | r)$ - the probability $t$ is the true taxon for read $r$

## Emu Pipeline

### Mapping

The EMU pipeline starts by taking the input reads $R$ and mapping them to the reference database $S$ using **minimap2**. This step generates the primary proper alignments.

Along with the alignments, we also receive the CIGAR string for each alignment. This supplies us with the information on all differences between the read and references they were aligned against also called the nucleotide **alignment type**.

### Initial probabilities

EMU takes the numerical values for each alignment type and determines the initial likelihoods for them. This is simply:

$$
P(c) =\frac{n_c}{\sum_{c\in C}n_c}
$$

where $P(c)$ is the likelihood for alignment type event, $n_c$ is the amount of events of alignment type $c$ amongst all the primary alignments:

<center>

| $X$ - mismatch | $I$ - insertion | $D$ - deletion | $S$ - softclip |
| :---: | :----: | :---: | :---: |

</center>

Similarly, EMU calculates the likelihood for each pairwise sequence alignment:

$$
P(r | s) = \prod_{c\in C} P(c) ^{n'_{c(r,s)}}
$$

where $n'_{(r,s)}$ is the normalized number of alignment type $c$ observed between alignment of $r$ and $s$.

The normalization of $n'_{(r,s)}$ is done as the length of the longest alignment for read $r$ divided by the length of the alignment:

$$
n'_{(r,s)} = n_{(r,s)} \frac{\max_{s'\in S}{\text{len}(r, s')}}{\text{len}(r,s)}
$$

::: note Probability & Likelihood
Probability is about data given a model. We talk about the probability of an *outcome*.
Likelihood is about the model given data. We talk about the likelihood of a *parameter*.
:::

If for specific pair no alignment is made, we set $P(r|s)=0$.

Since we are interested in the most-likely taxonomy of $r$ rather than the most-similar sequence $s$, we keep only the highest $P(r|s)$ for any $s$ with species-level taxonomy identification $t$. Thus, the alignment probability between each read $r$ and species-level taxonomy $t$ is calculated with 
$$
P(r|t) = \max_{s\in t}(\prod_{c\in C}P(c)^{n'_{c(r,s)}})
$$

where $s\in t$ represents all $s$ with taxonomy id $t$.

We also set initially

$$F(t)_{t\in T} := \frac{1}{|T|}$$

### 1. Expectation step (E-step)

Now, using Bayes' Theorem we can flip the conditional relation in probability $P(r|t)$ that we get a read $r$ given the sequence comes from $t$ to $P(t|r)$, the probability $t$ is the true taxon for read $r$:

$$
P(t|r) = \frac{P(r|t)P(t)}{P(r)} = \frac{P(r|t)F(t)}{\sum_{t\in T}P(r|t)F(t)}
$$

where we used the fact that $F(t)\equiv P(t)$ and $P(r)=\sum_{t\in T}P(r|t)F(t)$.

::: idea
This step down-weights alignments to species that are currently estimated to be rare in the sample. 

With error-prone Oxford Nanopore reads, a single read often looks like several different species. To fix this, the algorithm introduces and uses $F(t)$, the prior sample composition which represents the estimated relative abundance of species $t$ in the entire sample. (*"How likely is it to find this species in the sample at all?"*)

The "flip" calculates $P(t|r)$ (the probability the read actually belongs to species $t$) by multiplying the alignment score by the species abundance:

$$
P(t|r) \propto P(r|t) \times F(t)
$$

By flipping the probability, Emu penalizes the rare species. It assumes that an any random read is statistically much more likely to come from the "soup" of species we already know exists in the sample, rather than a species that is barely present.
:::

### 2. Maximization step (M-step)

Redistribute the sample composition $F$ based on the probabilities calculated in the E-step:

$$
L(R) = \sum_{r \in R} \log \left[ \sum_{s \in S} P(r|t) \cdot F(t) \right]
$$

::: idea
In many regression problems (like standard linear regression), we can invert a matrix and solve for the optimal weights *analytically* in a single step. This is not done in practice however as inverting matrix is a harder problem. 

The goal of Emu is to find the abundance profile $F$ that maximizes the **total log likelihood** of observing all reads $R$. The equation provided in the paper is:

$$L(R) = \sum_{r \in R} \log \left[ \sum_{s \in S} P(r|t) \cdot F(t) \right]$$

To solve this *analytically*, we would normally take the derivative of $L(R)$ with respect to $F(t)$, set it to zero, and solve for $F(t)$.

The problem, however, arises when we take the derivative of log of sums and we end up with:

$$
\frac{\partial L}{\partial F(t)}
= \sum_{r\in R} \frac{P(r\mid t)}{\sum_{s\in S} P(r\mid t)F(t)} = 0
$$

This is a set of non-linear equations that cannot be isolated algebraically. Since we cannot jump to the peak of the mountain (analytical solution) because the map is too complex (coupled non-linear equations), we use the Expectation-Maximization (EM) algorithm to climb the hill one step at a time.

- Step 1 (Expectation): Pretend we know the abundance $F$. Use it to softly assign reads to species.
- Step 2 (Maximization): Pretend these assignments are hard facts. Just count the reads to update $F$.
:::

### 3. Convergence step

To check for convergence we calculate the total log-likelihood $L(R)$ of the data given the model:

$$
L(R) = \sum_{r \in R} \log \left[ \sum_{s \in S} P(r|t) \cdot F(t) \right]
$$

If the increase in $L(R)$ compared to the previous iteration is greater than $0.01$ (is substantial), the loop repeats with updated $F$. Otherwise the redistribution is complete and we move on to the next step.

### Noise trimming

The probabilistic nature of the algorithm causes the vector $F$ to contain a tail of entries (but not in entry position meaning of the word) with small probabilities. This tail of false positives can be removed by setting a threshold under which any abundance will be set to $0$. The threshold is set so that if the probability $F(t)$ is calculated as 

$$
\frac{\text{\#reads from t}}{\text{\#total reads}}
$$

then the threshold is

- $1$ read from $t$ for small samples
- $10$ reads from $t$ for samples of $ >1000$

### Final pass

Emu performs one final round of abundance redistribution with the trimmed vector to produce the final community profile. In the software the resulting $F$ is exposed as the final sample composition estimation.


# ======================
# File: articles\compbio\journal\qna.md
# ======================

---
title: "Q&A"
layout: "base/base_article.njk"
homeTag: "compbio"
tags: "compbio_blog_2"
order : 1
---

# 1

Question: Why use the word 'taxonomy' when EMU specifically talks about the resolution of single species.
Answer: Because a reference genome in the database can be identified with higher level taxonomical classification than species, e.g. a genus.

2. what is the function len (of the alignment)

3\. why logarithm

# ======================
# File: articles\dipl\0_abstract.md
# ======================

---
title: "Abstract"
layout: "base/base_article.njk"
homeTag: "dipl"
tags: "infos"
order : 1
---

# Abstract

Volumetric data from medical scans (e.g. CT, MRI) is essential for clinical analysis, medical education, and patient communication. However, achieving photorealistic rendering of this data using traditional methods often either compromises real-time rendering times or requires expensive hardware, limiting its accessibility.

This thesis proposes a novel rendering framework to address this limitation. We will investigate the use of path tracing combined with splatting methods to render medical volumetric data. Together with targeted performance optimizations, we aim to achieve photorealistic and real-time results on consumer-grade GPUs.

The efficacy of the proposed method will be evaluated through its performance and visual fidelity will be benchmarked against traditional volume rendering techniques. The diagnostic quality of the resulting visualizations will be qualitatively assessed in consultation with medical professionals.

# ======================
# File: articles\dipl\0_details.md
# ======================

---
title: "Details"
layout: "base/base_article.njk"
homeTag: "dipl"
tags: "infos"
order : 2
---

# Details

## Master's Thesis

Department - Faculty  - Institute
: Faculty of Mathematics, Physics and Informatics - Department of Computer Science - Comenius University Bratislava

Supervisor
: doc. RNDr. Pavel Chalmovianský, PhD.

Title
: Modern Techniques for Medical Data Imaging on Consumer-Grade Hardware

Abstract
: Volumetric data from medical scans (e.g. CT, MRI) is essential for clinical analysis, medical education, and patient communication. However, achieving photorealistic rendering of this data using traditional methods often either compromises real-time rendering times or requires expensive hardware, limiting its accessibility.

: This thesis proposes a novel rendering framework to address this limitation. We will investigate the use of path tracing combined with splatting methods to render medical volumetric data. Together with targeted performance optimizations, we aim to achieve photorealistic and real-time results on consumer-grade GPUs.

: The efficacy of the proposed method will be evaluated through its performance and visual fidelity will be benchmarked against traditional volume rendering techniques. The diagnostic quality of the resulting visualizations will be qualitatively assessed in consultation with medical professionals.


# ======================
# File: articles\dipl\0_introduction.md
# ======================

---
title: "Introduction"
layout: "base/base_article.njk"
homeTag: "dipl"
tags: "miscs"
order : 2
---

# Introduction

TODO

intoroduction to the problem, possible solutions and current ideas of how to proceed

TODO

# ======================
# File: articles\dipl\0_literature.md
# ======================

---
title: "Literature"
layout: "base/base_article.njk"
homeTag: "dipl"
tags: "infos"
order : 3
---

# Literature

## Primary Literature

Optical properties of biological tissues: a review
: https://doi.org/10.1088/0031-9155/58/11/r37
```
@article{Jacques_2013,
doi = {10.1088/0031-9155/58/11/R37},
url = {https://doi.org/10.1088/0031-9155/58/11/R37},
year = {2013},
month = {may},
publisher = {IOP Publishing},
volume = {58},
number = {11},
pages = {R37},
author = {Jacques, Steven L},
title = {Optical properties of biological tissues: a review},
journal = {Physics in Medicine & Biology},
}
```

3D Gaussian Splatting for Real-Time Radiance Field Rendering
: https://doi.org/10.48550/arXiv.2308.04079
```
@misc{kerbl20233dgaussiansplattingrealtime,
      title={3D Gaussian Splatting for Real-Time Radiance Field Rendering}, 
      author={Bernhard Kerbl and Georgios Kopanas and Thomas Leimkühler and George Drettakis},
      year={2023},
      eprint={2308.04079},
      archivePrefix={arXiv},
      primaryClass={cs.GR},
      url={https://arxiv.org/abs/2308.04079}, 
}
```

Real-Time Monte-Carlo Path Tracing of Medical Volume Data
: [Local Copy](/resources/papers/orig_cr_mc.pdf)
```
@inproceedings{Engel2016,
  author    = {Engel, Klaus},
  title     = {Real-Time Monte-Carlo Path Tracing of Medical Volume Data},
  booktitle = {Proceedings of the GPU Technology Conference (GTC)},
  year      = {2016},
  address   = {San Jose Convention Center, San Jose, CA, USA},
  month     = {April 4--7},
  note      = {GPU Technology Conference},
}
```

Physically Based Rendering: From Theory to Implementation
: https://www.pbrt.org/
```
@book{Pharr2023,
  author    = {Pharr, Matt and Jakob, Wenzel and Humphreys, Greg},
  title     = {Physically Based Rendering: From Theory to Implementation},
  edition   = {4th},
  year      = {2023},
  publisher = {MIT Press},
  address   = {Cambridge, MA, USA},
  isbn      = {9780262048026},
  url       = {https://pbrt.org/}
}
```

## Other Literature
*(Subject to change)*
### Direct Volume Rendering

#### Articles

*   [https://cgl.ethz.ch/teaching/former/scivis\_07/Notes/stuff/StuttgartCourse/VIS-Modules-06-Direct\_Volume\_Rendering.pdf](https://cgl.ethz.ch/teaching/former/scivis_07/Notes/stuff/StuttgartCourse/VIS-Modules-06-Direct_Volume_Rendering.pdf)

### General Rendering

#### Books

*   Physically Based Rendering (PBR),
*   Real-Time Rendering, CRC

### Monte Carlo Path Tracing

#### Papers

*   Mathematical Basics of Monte Carlo Rendering Algorithms

### 3D Gaussian Splatting

*   [https://github.com/MrNeRF/awesome-3D-gaussian-splatting](https://github.com/MrNeRF/awesome-3D-gaussian-splatting) - Curated list of resources on 3DGS

#### Papers

*   [https://arxiv.org/pdf/2308.04079](https://arxiv.org/pdf/2308.04079) - Original 3DGS paper
*   [https://arxiv.org/pdf/2311.16043](https://arxiv.org/pdf/2311.16043) - Original Relightable 3D Gaussians
*   [https://arxiv.org/pdf/2311.17977](https://arxiv.org/pdf/2311.17977) - 3DGS with shading functions
*   [https://arxiv.org/pdf/2311.16473](https://arxiv.org/pdf/2311.16473) - Another Relightable 3DGS
*   [https://arxiv.org/pdf/2507.20512](https://arxiv.org/pdf/2507.20512) - Complex 3DGS shadowing
*   [https://arxiv.org/pdf/2412.19282](https://arxiv.org/pdf/2412.19282) - Reflective 3DGS

# ======================
# File: articles\dipl\0_motivation.md
# ======================

---
title: "Motivation"
layout: "base/base_article.njk"
homeTag: "dipl"
tags: "miscs"
order : 1
---

# Motivation
*(where i talk about why is something like this even needed)*


# ======================
# File: articles\dipl\0_roadmap.md
# ======================

---
title: "Roadmap"
layout: "base/base_article.njk"
homeTag: "dipl"
tags: "miscs"
order : 3
---

# Roadmap

![](/images/roadmap.svg)

*(PLACEHOLDER MD TEST)*
## Castle Building Roadmap

## Overview
A high-level, phase-driven roadmap for planning and executing a castle construction project. Adapt the timelines, resources and deliverables to your scope (historical reconstruction, fantasy build, real-world fortified manor, etc.).

## Vision & Objectives
- Primary vision: Build a durable, functional castle that meets safety, aesthetic and program requirements.
- Objectives:
    - Structural integrity and longevity
    - Defensible layout (if applicable)
    - Period-appropriate aesthetics / chosen style
    - Visitor access and safety
    - Budget and schedule adherence

## Success Criteria
- Structural inspection passed
- Key spaces (keep, great hall, gatehouse, towers) functional and approved
- Systems (water, drainage, basic utilities) operational
- Final budget within agreed variance
- Handover documentation and maintenance plan complete

---

## Phases & High-Level Timeline
1. Concept & Feasibility — 2–6 weeks
2. Site Survey & Permits — 4–12 weeks (parallel tasks possible)
3. Schematic & Detailed Design — 8–16 weeks
4. Site Preparation & Foundations — 4–12 weeks
5. Superstructure (walls, towers) — 12–36 weeks
6. Roof, Floors & Openings — 8–20 weeks
7. Interiors & Systems — 8–20 weeks
8. Defenses, Landscaping & External Works — 6–16 weeks
9. Commissioning & Inspections — 2–6 weeks
10. Handover & Maintenance Setup — 2–4 weeks

(Adjust durations according to scale and resources.)

---

## Example Milestones
- M1: Feasibility report approved
- M2: Planning permission granted
- M3: Foundation completed and inspected
- M4: External walls closed / weather-tight
- M5: Roof complete
- M6: Systems commissioned
- M7: Final inspection passed
- M8: Handover complete

Each milestone should include acceptance criteria, owner, target date, and required documents.

---

## Roles & Responsibilities
- Project Sponsor — funding and strategic decisions
- Project Manager — schedule, budget, coordination
- Lead Architect / Historian — design and authenticity
- Structural Engineer — foundations and load-bearing elements
- Site Manager / Contractor — daily site operations
- MEP Engineer — utilities and systems
- Safety Officer — site safety and compliance
- QA / Inspector — quality checks and sign-offs

---

## Resource Plan
- Materials: stone/brick, lime mortar, timber, metalwork, roofing materials
- Equipment: excavation, scaffolding, lifting equipment
- Labor: masons, carpenters, laborers, specialists
- Consultants: archaeologist (if needed), conservation specialist, planners
- Budget categories: design, permits, materials, labor, contingencies, professional fees

---

## Risk Register (sample entries)
- Ground conditions worse than expected — mitigant: pre-construction geotechnical survey, contingency budget
- Permit delays — mitigant: early engagement with authorities, buffer in schedule
- Material shortages / price increases — mitigant: early procurement, alternate suppliers
- Weather impacts — mitigant: phased scheduling, temporary weather protection

---

## Deliverables per Phase
- Concept: vision statement, feasibility study, preliminary cost estimate
- Design: schematic plans, detailed drawings, structural calculations, specification
- Construction: daily logs, inspection reports, material certificates
- Commissioning: system test reports, safety certificates
- Handover: as-built drawings, O&M manuals, maintenance schedule

---

## Communication Plan
- Weekly site report (to stakeholders)
- Bi-weekly progress meeting (project team)
- Monthly executive summary (sponsor)
- Change control process: log changes, impact assessment, formal approval

---

## Acceptance Checklist (before Handover)
- Structural certificate obtained
- Fire & life-safety checks passed
- Utilities functional (water, power where applicable)
- Drainage and erosion control verified
- Accessibility / paths inspected
- As-built drawings and documentation delivered

---

## Maintenance & Lifecycle
- Routine inspections (annual)
- Masonry repointing schedule (as required)
- Roof and drainage maintenance (bi-annual)
- Record-keeping for repairs and conservation

---

## Templates & Notes
- Milestone template: Title, Owner, Target date, Acceptance criteria, Status
- Task template: ID, Description, Assignee, Start/End, Dependencies, Estimate, Status
- Change request template: ID, Description, Impact (schedule/budget/scope), Decision

---

## Appendix
- Glossary of terms
- Reference standards and local regulations
- Contact list (authorities, suppliers, consultants)

Customize this roadmap to your project's scale, style and legal context. Use the milestones and templates to convert the plan into a detailed project schedule and task list.

# ======================
# File: articles\dipl\0_thoughts.md
# ======================

---
title: "Thoughts"
layout: "base/base_article.njk"
homeTag: "dipl"
tags: "miscs"
order : 4
---


# Thoughts

*(30.10.2025)*

will contain a dynamically growing collection of thoughts with dates to track mental process and progress throughout the writing

*(1.11.2025)*
![](/images/thoughts_1.svg)

# ======================
# File: articles\dipl\1_medical_scans.md
# ======================

---
title: "Medical Scans"
layout: "base/base_article.njk"
homeTag: "dipl"
tags: "chapters"
order : 1
---

*(30.10 - )*

# Medical Scans

This chapter serves as a introduction to the concepts of medical scans. The bread and butter of our thesis as the source of input data.

## CT Scans

CT scan
: computed tomography scan

In this section we will look at the fundamentals of ct scans. As our primary source of data it is important to get a sense of the historical motivation behind the technology and the science that drove it.

We will briefly cover its beginnings, advances and the current state-of-art solutions that maximize the quality of the resulting scan but at the same time minimize the exposure to radiation. Then, briefly, we will examine the physics and mathematics behind the process of acquiring and processing ct scans so that a full and informative picture can be made as to where our sources come from.

![Alt text](/images/1_1.svg)


# ======================
# File: articles\dipl\1_volume_reconstruction.md
# ======================

---
title: "Volume Reconstruction"
layout: "base/base_article.njk"
homeTag: "dipl"
tags: "chapters"
order : 2
---



# ======================
# File: articles\dlcv\homework1.md
# ======================

---
title: "Homework 1"
layout: "base/base_article.njk"
homeTag: "dlcv"
tags: "dlcvs"
order : 1
---

# Homework 1: Cifar10 CNN

## Assignment

The goal of this homework is to:

- Design a basic CNN and train on Cifar10 dataset
- Improve the model iteratively and benchmark each iteration

### What is Cifar10

CIFAR-10 is a collection of 60,000 32x32 **color images** divided into 10 mutually exclusive classes. The dataset is split into a training set of 50,000 images and a test set of 10,000 images. The 10 classes are: airplane, automobile, bird, cat, deer, dog, frog, horse, ship, and truck.

## Solution

### Setting

For this project, we will also try to learn how to use Google Cloud virtual machine for compute. Because the virtual machine uses os image with `python 3.12` and `pytorch 2.7.` we will be using those versions.

We will be using uv for project management.

The package name of our project will be `cifar10_cnn` and the structure is:

- Core package contains the training module
- Subpackage `models` contains the base model and every upgrade iteration
- Subpackage `tests` contains testing
- Subpackage `utils` contains data loaders

### Dataset Loaders

Function `get_loaders(batch_size, augment=False, first_1000=False)` returns `DataLoader` objects from `torch.utils.data`:

- the `train_loader` filled with training data
- the `val_loader` filled with validation data

The data can be transformed between loading from builtin Cifar10 object to the loaders:

- `batch_size` - self explaining
- `augment` - image augmentation is handled trough tensor transformation by `CIFAR10` object
- `first_1000` - allows for fast prototyping via `Subset` object

### Input data

Images with resolution of 32x32 and 3 channels. 10 classes. `(image, class)`

### Training

The training loop is classic pytorch template loop:

- **For all epochs do**
    - Train one epoch on training data
    - Evaluate the epoch on validation data
- **For each epoch do**
    - Set model to training mode - `model.train()`
    - **For each batch in loader do** (e.g. `torch.Size([64, 3, 32, 32])`)
        - Send the data to available device (CPU, GPU if CUDA)
        - Do one forward pass on the data
        - Calculate loss
        - Compute statistics

### Base Model

The base model `class Cifar10_CNN_Base(nn.Module):` uses the Adam optimizer and ReLU activations per the assignment.

The model consists of 3 feature-extraction layers with kernel size 3 and standard padding of 1:

| Layer | InOut / Size | Result Data Dimension |
| :---        |    :----:   |          ---: |
| Input | - | `3x32x32` |
| 2D Convolutional | `3, 32` | `32x32x32` |
| 2D Max Pool | `2` | `32x16x16` |
| 2D Convolutional | `32, 64` | `64x16x16` |
| 2D Max Pool | `2` | `64x8x8` |
| 2D Convolutional | `64, 128` | `128x8x8` |
| 2D Max Pool | `2` | `128x4x4` |

And of two classification layers that take in flattened output of the convolution module:

| Layer | InOut / Act | Result Data Dimension |
| :---        |    :----:   |          ---: |
| Input | - | `128x4x4` |
| Flatten | - | `2048` |
| ---        |    ---   |          --- |
| Input | - | `2048` |
| Linear | ReLU | `2048x256` |
| Linear | - | `256x10` |

The output is raw logit representation of the classes logistic distribution.



# ======================
# File: articles\ds\cache.md
# ======================

---
title: "Cache"
layout: "base/base_article.njk"
homeTag: "ds"
tags: "datastructures"
order : 1
---

### CPU cache

CPU cache is a small, high-speed memory that is physically located near the CPU. It sotred frequently used data and instructions to reduce the time it takes the processor to access them from other memory, like the main memory (RAM).

Modern processors have multiple cache levels. These levels interact with each other and with the CPU based on the specific CPU architecture.

The main properties of cache are
- cache size
- cache block size
- the number of blocks in a set
- the cache set replacement policy
- the cache write policy

### Cache Loading

When a CPU wants to access a data stored in the main memory it first loads the data into the cache. This loading, however, only happens in chunks called **cache lines**. Typical cache line is 64 bytes (on modern x86-64 CPUs). This means that the cpu loads the amount of data it wants to access and the next however bytes to load full 64 bytes.

This also means that we may get unrelated memory in the cache to our interest. This is normal since cache behavior is not observable at the semantic level, it happens automatically and we need to work with it in order to maximize the performance of our program.

- L1 cache access: ~0.5ns
- L2 cache access: ~3ns
- L3 cache access: ~15ns
- RAM access: ~100ns
- SSD access: ~10ms (10000ns)

### Cache-Friendliness

Even a single trip to DRAM is about 200 CPU cycles. We want our programs to work with the CPU's natural prefetching and caching mechanism. Once the data is in cache, the operations are extremely fast so we want to design our algorithms so that we minimize:

#### Cache misses

Cpu fails to find the data it needs in cache and has to load another cache line. We minimize this waste by designing layouts so that elements of the data structure fit inside the cache lines.

- grouping 'hot' (frequently accessed) fields together
- using structs of arrays instead of arrays of structs
- *not* placing rarely used fields next to hot data
- *not* storing 32-byte pointers next to 8-byte ints

#### Pointer chasing

Every pointer-following step risks crossing to a new cache line. Meaning:

- hash maps with chained buckets perform poorly
- tree structures are slow
- `absl::flat_hash_map` hash maps put keys and values contiguously in arrays and are *fast*



# ======================
# File: articles\ds\flat_hash_map.md
# ======================

---
title: "flat_hash_map"
layout: "base/base_article.njk"
homeTag: "ds"
tags: "datastructures"
order : 5
---

*Based on: CppCon 2017: Matt Kulukundis “Designing a Fast, Efficient, Cache-friendly Hash Table, Step by Step”*

## Designing and Implementing flat_hash_map

The hash table described by Matt Kulukundis in his presentation at CppCon 2017 is a **swiss table** (designed in Zürich by Abseil) family of hash tables design. They are meant to replace `std::unordered_set` and `std::unordered_map`.

The **flat** variant of the swiss table is a open-addressed hash table with extra metadata to hold information about the slots, allowing for fast lookup, delete and insert times.

### all items are stored in a single backing array

items:
: the values distributed according to the hash values of their keys modulo table size

In `flat_hash_map` all items are stored in a single array, also called **backing array**. We call each location in the array a **slot**. This sets the scene for cache-friendliness.

### the backing array is divided into groups of 16 slots

The defining feature of swiss tables is that it utilizes SIMD to effectively compare metadata when performing an operation on the table. The result of such comparison is a 16-bit long word that acts as a mask for our 16 slots in a group.

### metadata is tightly packed

The metadata uses 8 bits (a WHOLE BYTE) for each element and is tightly packed in memory. This means we have 16 bytes of tightly packed metadata.

- Store 1 bit (msb) as a flag:
    - 0 - full slot, follows by 7 bits of hash
    - 1 - non-full slot
- Store 7 bits of hash code in the least significant bits

**If** a slot is full, the first bit will be 0 followed by the hash value:
`0b0xxxxxxx`

**Else if** a slot is not full but was never occupied, then the value is
`kEmpty = -128` or `0b10000000`

**Else if** a slot is not full but was once occupied and deleted, then it is a tombstone with a value
`kDeleted = -2` or `0b11111110`

**Else** the slot is a padding, allowing us to stop scanning metadata for table scan and uses
`kSentinel = -1` or `0b11111111`

### the hash value is thus divided into two

Every time we hash a key, we divide the hash value into two parts:
- **H1** - 57 bits and determines the position in the array
- **H2** - 7 bits and is used in the metadata

:::note Entropy
Since we split the hash value into two distinct parts we need a hash function that distributes entropy evenly. This prevents collisions in either H1 or H2 since unevenly distributed entropy makes one of those two collision-prone.
:::

**Implementation**

For H1 we just shift 7 bits, leaving the 57 msb bits. For H2 we filter out the msb bits with `01111111` AND operation, leaving us with 7 lsb. 

```cpp
size_t H1(size_t hash) { return hash >> 7; }
ctrl_t H2(size_t hash) { return hash & 0x7F; }
```

### find operation

**Sequential**

- when looking for a key in the table we first compute the hash value of the key
- the hash value is then split into H1 and H2
- H1 is ran through modulo `num_groups` to find the index of the group it should fall into
- sequentially iterate over all elements in the group
- **if is full**: compare H2 hash value
    - **if same**: hit
    - **if not same**: move to the next slot
- **if is not full**: move to the next slot

**SIMD-based**

- find the group index
- compare ALL H2 entries in metadata
- jump to the matching element

::: idea
**SIMD Filtering** is the key to performance here. We check 16 slots at once rather than iterating one by one.
:::

**Implementation**

The 16 slot group allows us to compute the H2 values extremely fast.

:::note SSE
Streaming SIMD Extensions (SSE) is a single instruction, multiple data (SIMD) instruction set extension to the x86 architecture.
:::

The `find` function is:

```cpp
iterator find(const K& key, size_t hash) const {
    size_t group = H1(hash) % num_groups_;
    while (true) {
        Group g{ctrl_ + group * 16};
        for (int i : g.Match(h2)) {
            if (key == slots[group * 16 + i])
                return iterator_at(group * 16 + i);
        }
        if (g.MatchEmpty()) return end();
        group = (group + 1) % num_groups_;
    }
}
```

:::note Compiler Details
The `if (key == slots[group * 16 + i])` equality operator is ALMOST always true at least once, since we already know the kay should fall into that group as we are comparing the H2 hashes. This allows for `predict true`.
:::

:::note Full Groups
If `(g.MatchEmpty()) return end();` is false, we hit an empty, never filled slot and the search may stop since it means no other collisions and potential matches occur after that. If it is true for the whole group we must probe to the next group. This almost never happens pre Matt Kulukundis.
:::

The SIMD `match` function is:

```cpp
BitMask<unit32_t> Match(h2_t hash) const {
    auto match = _mm_set1_epi8(hash);
    return BitMask <uint32_t>(
        _mm_movemask_epi8(_mm_cmpeq_epi8(match, crtl))
    );
}
```

Where:

- `_mm_set1_epi8(arg)` initializes array of 16 elements each of size one byte with value of argument
- `_mm_cmpeq_epi8(arg1, arg2)` compares the arguments byte wise and returns `FF` in place of match and `00` otherwise
- `_mm_movemask_epi8(arg)` takes 16 byte array and squished it into 16 bits (according to the msb of the array)

### some naming

A group consists of the backing array and the metadata array. A position in the backing array is slot and position in the metadata is called **control**. A table has $N$ groups.

### erasing is easy

```cpp
void erase(iterator it) {
    --size_;
    Group g{(it.ctrl_ - ctrl_) / 16 * 16 + crtl_};
    *it.ctrl_ = g.MatchEmpty() ? kEmpty : kDeleted;
    it.slot_.~K()
}
```

# ======================
# File: articles\ds\hashing.md
# ======================

---
title: "Hashing"
layout: "base/base_article.njk"
homeTag: "ds"
tags: "datastructures"
order : 2
---

## Hashing

Hashing is a process during which an input, called the **key**, gets transformed into a fixed-size output, a **hash value** using a **hash function**.

### Compression of Information

Information theory treats a hash function as a function that compresses information into fewer bits.

- SHA-256 maps to 256 bits
- a typical hash table maps to 32 or 64 bits
- bucket hash table maps to a few bits after modulo

Since this hash mapping reduce the number of bits, information is necessarily lost. This causes an events called **collisions** where two keys map to the same hash value.

### Good Hash Function

A good hash function exhibits high level of randomness.

- every bit of the output depends on each bit of the input
- output bits behave statistically independently
- for any two keys, the probability they map to the same hash value is small, ideally uniform

### Data Hashing

The goal of hashing in computer science applications is to place data into a fixed-sized structure efficiently. This includes:

- hash tables
- bloom filters
- hash-based file systems
- symbol tables
- caches
- cryptography

This is achieved by interpreting the hash value of a piece of data as the index in that data structure. 

```python
h = hash(key)
index = h mod table_size
```

### The Need for Speed

For arbitrary data, hashing is the only known practical method to achieve constant time lookup. This means that hash tables allow us to access and element in expected constant time.

- modern workloads require billions of lookups per second (google)

### Distributed Systems

Having a consistent hashing system allows distributed systems to unify in data storage and handle requests among clusters. Hashing eliminates the need for central command unifying local data storage systems.






# ======================
# File: articles\ds\hash_functions.md
# ======================

---
title: "Hash Functions"
layout: "base/base_article.njk"
homeTag: "ds"
tags: "datastructures"
order : 3
---

## Hash Functions

> TODO: remove nutshell

::: note In a Nutshell
Put this key into a slot X.
:::

A hash function is a deterministic mapping:

$$
h:U \to [0, m-1]
$$

Where:
- **U** is the universe of possible key
- **m** is the number of slots in a hash table (buckets)
- the functional value **h(k)** is the chosen index from $[0,m-1]$

### Universe of Keys

The universe of key is everything we could ever hash. For example

- all possible 64-bit ints
- all possible strings
- all possible memory addresses

The problem in reality is that the universe is way bigger than the hash table. This means collisions are inevitable. This plays a role in choosing the right hash function.

### Choosing the Hash Function

- if hashing behaves unevenly $\implies$ some slots get overloaded
- overloading $\implies$ lookup costs rise
- potentially: lookup costs rise $\implies$ attacker can force $O(n)$ times

So we want the hash function to at least *look* random.

> TODO hash function families


# ======================
# File: articles\ds\hash_tables.md
# ======================

---
title: "Hash Tables"
layout: "base/base_article.njk"
homeTag: "ds"
tags: "datastructures"
order : 4
---

## Hash Tables

::: note In a Nutshell
Hash tables are space-time tradeoff. Infinite memory means the entire key can be used directly as an index. Infinite time means values can be stored without regard for their keys.
:::

Hash tables are the way to implement the associative array abstract data structure. A key is hashed using a hash function. The hash value is used to index the key in a memory. The key may be associated with a value (maps) or may not (sets).

- `len(table)` is small relative to `len(hashable_keys)` $\implies$ collisions are inevitable

The art of hash tables is the art of collision resolution.

- pointer solution: chaining
- index computing solution: probing

### Chaining Hash Tables

Each slot contains a list of items.

- C++ `std::unordered_map`
- Java `HashMap`

### Probing

The chaining hash table solves collisions by introducing pointers. The hash table slot points to the start of the list containing all the collided keys. Additionally, to be able to grow the slot at the start, we need to keep a dummy.

> TODO why not vectors for locality?

Pointers are an enemy of cache-friendliness. We want to make sure cache lines contain the collided sequence of keys. This also allows for SIMD-ing the probing.

A collision triggers a computation of new index:

- linear probing: the new index is the collided one incremented by 1
```mathematica
h, h+1, h+2, h+3, ...
```
- quadratic probing
```mathematica
h, h+1^2, h+2^2, h+3^2, ...
```
- double hashing
```mathematica
h + i * h2(key)
```

The con of probing is the difficulty when deleting an entry.

### Modern Probing

- Google SwissTable / Abseil `flat_hash_map` (metadata + SIMD acceleration)
- Rust `HashMap` (SwissTable)
- Robin Hood hashing (balances probe lengths)




# ======================
# File: articles\mathematics\probstat\conditional_probability.md
# ======================

---
title: "Conditional Probability"
layout: "base/base_article.njk"
homeTag: "mathematics"
tags: "mathematics_prob"
order : 2
---

We are Bayesian statisticians.

We imagine a world where we know it may rain but it doesn't have to. We also know the grass may get wet but it doesn't have to.

We think nothing else can happen. Everything except raining and grass getting wet has a zero probability of happening.

**Outcomes:**
- Let $Pr(a)$ be the probability of outcome $a$ called: **it rains and the grass is wet**.
- Let $Pr(b)$ be the probability of outcome $b$ called: **it rains and the grass is not wet**.
- Let $Pr(c)$ be the probability of outcome $c$ called: **it doesn't rain and the grass is wet**.
- Let $Pr(d)$ be the probability of outcome $d$ called: **it doesn't rains and the grass is not wet**.

**Events:**
- Event called: **it rains** - a subset $E_{\text{rain}} = (a, b)$
- Event called: **it doesn't rain** - a subset $E_{\text{no rain}} = (c, d)$
- Event called: **the grass is wet** - a subset $E_{\text{grass wet}} = (a, c)$
- Event called: **the grass is not wet** - a subset $E_{\text{grass not wet}} =(b, d)$
- *Plus the four above*

We are tasked with defining this: 

<center>

***"what is the probability that the grass is wet if it is given that it rains"***

</center>



\
It would be sensible to ask this question: what is the difference between the 'probability that the grass is wet and that it rains' and the 'probability that the grass is wet given it rains'? 

The first case is asking what is the probability of the intersection of events 'it rains' and 'grass is wet' which is the outcome $A$ with probability $P(a)$.

The second case is asking what is the probability of an event 'grass is wet' when any other outcome can come only from the event 'it rains'. Meaning, when fixing any event trough the statement: 'it is given that...', we create a new space of all possible outcomes - those that can occur only in the event we are taken as given. 

In our case, giving 'it rains', we limit ourselves only to outcomes in event $E_{rains}$: $(a, b)$.

We, however, did not touch the function or *parameters* of the world that determine the values of probabilities. This means, if no other outcome is possible, we would get $P(a)+P(b)\neq 1$. The probabilities of $P(a)$ and $P(b)$ must thus be recalculated as those are the only two outcomes in our new universe of outcomes. Let the probability be $P'$.

The relation ship between the remaining outcomes has not changed. If one was twice as probable as the other one, that ratio must be conserved.

We want to scale the probabilities to achieve the normalization condition.

$$
P'(a) = \alpha P(a) \\
P'(b) = \alpha P(b) \\
\alpha P(a) + \alpha P(a) = 1\\
\alpha = \frac{1}{P(b) + P(b)}
$$

From axiom of probability addition we may call the probability $P(a) + P(b)$ the probability of event 'it rains': $P(E_{rains})$.

The new probabilities are thus $P'(a) = \frac{P(a)}{P(E_{rains})}$ and $P'(b) = \frac{P(b)}{P(E_{rains})}$.

If we denote $P'(a)\equiv P(a|E_{rains})$ and $P'(b)\equiv P(b|E_{rains})$ we have:

$$
P(a|E_{rains}) = \frac{P(a)}{P(E_{rains})}
$$
$$
P(b|E_{rains}) = \frac{P(b)}{P(E_{rains})}
$$

Which is the definition of **Conditional Probability**:
$$
P(X \mid Y) = \frac{P(X \cap Y)}{P(Y)}
$$

---

More straight forward way is this.

Consider a space of possible outcomes $\Omega$. A probability of a event $A$ occurring is defined (by the way of measure) as

$$
Pr(A) = \frac{|A|}{|\Omega|}
$$

Now, assigning a probability that event (or some event from events) $A$ happened given that event (or some event from events) $B$ happened is saying that some event $C$ happened and $C$ must be a possible event in $A$ and $B$. 




# ======================
# File: articles\mathematics\probstat\foundations.md
# ======================

---
title: "Foundations of Probability"
layout: "base/base_article.njk"
homeTag: "mathematics"
tags: "mathematics_prob"
order : 1
---

Talking about probability is talking about assigning a value to some elements of sets, or to some subsets of sets.

A **outcome** is an element. Outcomes must be mutually exclusive. **No two outcomes can happen at the same time.**
An **event** is a subset with outcomes as its elements.

Both the value and the sets must fulfill some conditions to allow this assignment to be without contradictions:

- The set of all possible outcomes $\Omega$ can be whatever set.
- The set of all event $F$ must form a $\sigma$**-algebra**.
- The probability is a measure that assigns to each event $E\in F$ its probability value $P(E)$ (or $Pr(E)$)
- It is a convention that $P(\Omega)=1$
- For general uncountable sets, we need specific $\sigma$-algebra called the **Borel algebra**

We assign probabilities to events. A single outcome can be an event.

For all events, their probabilities are non-negative values.

The probability of an event $A$ happening **or** event $B$ happening (the probability of situation where at least one happens) is the sum of their respective probabilities.

It is important to distinguish between colloquial naming and mathematical definitions.

::: note Example
Let $\Omega$ be the set of outcomes:
- $A$ and name it: It rains and the grass is wet
- $B$ and name it: It rains and the grass is not wet
- $C$ and name it: It doesn't rain and the grass is wet
- $D$ and name it: It doesn't rain the grass is not wet

Let $F$ be the set of events:
- It rains: a subset of $A, B$
- It doesn't rain: a subset of $C, D$
- The grass is wet: a subset of $A, C$
- The grass is not wet: a subset of $B, D$
- Plus the four above
:::




# ======================
# File: css\banner.css
# ======================

:root {
    --banner-bg: #FFD54F;
    --banner-text: #000000;
}

/* Construction Banner */
.construction-banner {
    width: 100%;
    background-color: var(--banner-bg);
    color: var(--banner-text);
    display: flex;
    align-items: center;
    justify-content: center;
    padding: 12px 0;
    font-family: 'Helvetica Neue', Helvetica, Arial, sans-serif;
    font-size: 1rem;
    letter-spacing: 0.5px;
    box-shadow: 0 2px 6px rgba(0, 0, 0, 0.1);
    border-bottom: 1px solid rgba(0, 0, 0, 0.1);
    
    /* Positioning */
    position: fixed;
    top: 0;
    left: 0;
    z-index: 1000;
}

.construction-banner img {
    height: 35px;
    margin-right: 10px;
}

.construction-banner span {
    font-weight: 500;
}

# ======================
# File: css\katex.css
# ======================

.katex {
  font-size: 1.4em;
}

/* style block math */
.katex-display {
  margin: 1.8em 0;
}

# ======================
# File: css\markdown-containers.css
# ======================

/* ================
   GitHub-style Callouts
   ================ */

/* Base Callout Style */
.callout {
    border-left: 4px solid #ccc;
    background-color: #f9f9f9;
    padding: 1rem;
    margin: 1.5rem 0;
    border-radius: 0 4px 4px 0;
}

.callout-title {
    font-weight: bold;
    margin-top: 0;
    margin-bottom: 0.5rem;
    display: flex;
    align-items: center;
}

/* --- Standard Types --- */
.callout-note { border-color: #0969da; background-color: #f0f6fd; }
.callout-note .callout-title { color: #0969da; }

.callout-warning { border-color: #bf8700; background-color: #fff8c5; }
.callout-warning .callout-title { color: #bf8700; }

/* --- YOUR NEW CUSTOM BLOCKS --- */

/* 1. The "Core Idea" (Green) */
.callout-idea {
    border-color: #2ea44f;      /* Green Border */
    background-color: rgb(230, 255, 236);  /* Light Green Background */
}
.callout-idea .callout-title::before {
    content: "💡 ";             /* Add an icon via CSS */
    margin-right: 8px;
}

/* 2. The "Math Definition" (Formal Look) */
.callout-math {
    border: 1px solid #444;     /* Full border, not just left */
    border-left: 6px solid #444;
    background-color: #fff;
    box-shadow: 3px 3px 0px rgba(0,0,0,0.1);
    font-family: "Times New Roman", serif; /* Formal font */
}
.callout-math .callout-title {
    text-transform: uppercase;
    letter-spacing: 1px;
    border-bottom: 1px solid #eee;
    padding-bottom: 5px;
}

/* 3. The "Concept Definition" */
.callout-concept {
    border-color: #0969da;     /* Full border, not just left */
    background-color: #f0f6fd;;
}

# ======================
# File: css\page.css
# ======================

/* =============================
   Styles for markdown
   Based on github-markdown-light 
============================= */

OMITTED TO SAVE ON TOKENS

# ======================
# File: css\section.css
# ======================

/* =========================================
   1. CSS VARIABLES (THEME CONFIGURATION)
   ========================================= */
:root {
    /* Sidebar Dimensions */
    --sidebar-width-collapsed: 20px;
    --sidebar-width-expanded: 220px;
    
    /* Sidebar Colors */
    --sidebar-bg: #333333;
    --sidebar-item-bg: #444444;
    --sidebar-item-hover: #555555;
    --sidebar-text: #ffffff;
    --sidebar-highlight: #007bff;      /* Primary Action Color */
    --sidebar-highlight-hover: #0056b3;


    /* Transitions */
    --transition-speed: 0.25s;
}

/* =========================================
   2. SIDEBAR CONTAINER & LAYOUT
   ========================================= */


/* =========================================
   5. GENERAL PAGE LAYOUT & TYPOGRAPHY
   ========================================= */

body {
    font-family: sans-serif;
    display: flex;
    flex-direction: column;
    justify-content: center;
    align-items: center;
    min-height: 100vh;
    margin: 0;
    padding-top: 50px; /* prevent content from hiding under the banner */
}

main { 
    display: flex;
    flex-direction: column;
    justify-content: center;
    align-items: center;
}

/* Link Lists (section Content) */
.link-list {
    list-style: none;
    padding: 0;
    text-align: center;
}
.link-list li {
    margin: 15px 0;
}
.link-list a {
    font-size: 1.5em;
    color: var(--sidebar-highlight);
    text-decoration: none;
}
.link-list a:hover {
    text-decoration: underline;
}

/* Text Utilities */
.centered-text {
    display: block;          
    text-align: center;      
    white-space: normal;     
    word-wrap: break-word;   
    max-width: 100%;         
    overflow-wrap: anywhere; 
}

.links-line {
    border: 3px solid gray;
    width: 150px;
    margin: 20px auto;
    border-radius: 2px;
}



# ======================
# File: css\sidebar.css
# ======================

:root {
    /* Sidebar Dimensions */
    --sidebar-width-collapsed: 20px;
    --sidebar-width-expanded: 220px;
    
    /* Sidebar Colors */
    --sidebar-bg: #333333;
    --sidebar-item-bg: #444444;
    --sidebar-item-hover: #555555;
    --sidebar-text: #ffffff;
    --sidebar-highlight: #007bff;      /* Primary Action Color */
    --sidebar-highlight-hover: #0056b3;
}

.sidebar {
    position: fixed;
    top: 0;
    left: 0;
    height: 100vh;
    width: var(--sidebar-width-collapsed);
    background: var(--sidebar-bg);
    z-index: 9998;
    overflow: hidden; /* Hides internal content when collapsed */
    transition: width var(--transition-speed) ease;
    
    /* FLEXBOX SETTINGS (Scrollable Top + Fixed Bottom) */
    display: flex;
    flex-direction: column; 
    justify-content: space-between;
}

/* Hover expansion */
.sidebar:hover {
    width: var(--sidebar-width-expanded);
}

/* =========================================
   3. SIDEBAR SECTIONS
   ========================================= */

/* --- A. The Scrollable Top Area --- */
.sidebar-scroll-area {
    flex-grow: 1;      /* Takes up all available space */
    overflow-y: auto;  /* Adds scrollbar only if needed */
    overflow-x: hidden;
    padding-top: 40px; /* Space for the "MENU" label at the top */
    
    /* Firefox Scrollbar Styling */
    scrollbar-width: thin;
    scrollbar-color: var(--sidebar-item-hover) var(--sidebar-bg);
}

/* Webkit Scrollbar Styling (Chrome/Safari/Edge) */
.sidebar-scroll-area::-webkit-scrollbar {
    width: 6px;
}
.sidebar-scroll-area::-webkit-scrollbar-track {
    background: var(--sidebar-bg);
}
.sidebar-scroll-area::-webkit-scrollbar-thumb {
    background-color: var(--sidebar-item-hover);
    border-radius: 3px;
}

/* --- B. The Fixed Bottom Section --- */
.sidebar-bottom-action {
    flex-shrink: 0; /* Prevents this section from shrinking */
    padding-bottom: 20px;
    background: var(--sidebar-bg); /* Ensures it covers scrolling content behind it */
    border-top: 1px solid var(--sidebar-item-bg); /* Optional separator */
    margin-top: auto;
}

/* =========================================
   4. SIDEBAR ELEMENTS (LABELS & LINKS)
   ========================================= */

/* Vertical label shown only when collapsed */
.sidebar-label {
    position: absolute;
    top: 50%;
    left: 0;
    transform: rotate(-90deg) translate(-50%);
    transform-origin: left top;
    font-size: 14px;
    font-weight: bold;
    color: var(--sidebar-text);
    letter-spacing: 2px;
    opacity: 1;
    pointer-events: none;      /* Don’t block hover */
    transition: opacity 0.2s ease;
}

/* Hide the label when expanded */
.sidebar:hover .sidebar-label {
    opacity: 0;
}

/* General list styling */
.sidebar ul {
    list-style: none;
    padding: 0; 
    margin: 0;
}

.sidebar li {
    margin: 16px 0;
}

/* Link Text Styling */
.sidebar a {
    text-decoration: none;
    color: var(--sidebar-text);
    font-size: 1rem;
    opacity: 0;                         /* Hidden when collapsed */
    white-space: nowrap;
    transition: opacity 0.2s ease 0.1s;
}

/* When expanded, make links visible */
.sidebar:hover a {
    opacity: 1;
}

/* Box Styling (The clickable buttons) */
.sidebar-box {
    display: block;
    padding: 10px 12px;
    margin: 6px 14px;
    background: var(--sidebar-item-bg);
    border-radius: 6px;
    color: var(--sidebar-text);
    text-decoration: none;
    opacity: 0;
    transform: translateX(-10px);
    transition:
        opacity 0.2s ease,
        transform var(--transition-speed) ease,
        background 0.2s ease;
}

/* Show + animate boxes when expanded */
.sidebar:hover .sidebar-box {
    opacity: 1;
    transform: translateX(0);
}

/* Hover highlight */
.sidebar-box:hover {
    background: var(--sidebar-item-hover);
}

/* Active State (Add this class to your active link in HTML) */
.sidebar-box.active {
    border-left: 3px solid var(--sidebar-highlight);
    background: #222; 
}

/* Special Button (LLM Chats) */
.special-action {
    background-color: var(--sidebar-highlight);
    font-weight: bold;
    text-align: center;
}
.special-action:hover {
    background-color: var(--sidebar-highlight-hover);
}

# ======================
# File: css\widgets.css
# ======================

/* =========================================
   6. ANIMATIONS & DECORATIONS
   ========================================= */

.scroll-arrow {
    position: fixed;
    right: 20px;
    bottom: 20px;
    width: 40px;
    height: 40px;
    background-color: gray;
    /* Keeps your existing triangle shape */
    clip-path: polygon(50% 100%, 0 0, 100% 0); 
    z-index: 9999;
    
    /* Animation Definition */
    animation: bounce 1.2s infinite;
    
    /* Transition for the fade out effect */
    opacity: 1;
    transition: opacity 0.5s ease;
    cursor: pointer; /* Optional: indicates it's interactive */
}

/* The class added by JS to hide it */
.scroll-arrow.fade-out {
    opacity: 0;
    animation: none; /* Stop bouncing while fading */
    pointer-events: none; /* Prevent clicking while invisible */
}

/* Reuse your existing keyframes */
@keyframes bounce {
    0%, 100% { transform: translateY(0); }
    50% { transform: translateY(-10px); }
}


/* =========================================
   7. FLOATING ELEMENTS
   ========================================= */

.floating-home-btn {
    position: fixed;
    bottom: 20px;
    left: 50%;
    transform: translateX(-50%);
    
    /* Box Model */
    padding: 8px 14px;
    border-radius: 6px;
    
    /* Visuals */
    background-color: var(--sidebar-highlight); /* #007bff */
    color: #ffffff;
    text-decoration: none;
    box-shadow: 0 2px 6px rgba(0,0,0,0.2);
    opacity: 0.7;
    z-index: 9999;
    
    /* Typography */
    font-size: 14px;
    font-family: 'Helvetica Neue', Helvetica, 'Segoe UI', Arial, freesans, sans-serif;
    
    /* Transitions */
    transition: opacity 0.3s, background-color 0.2s;
}

.floating-home-btn:hover {
    opacity: 1;
    background-color: var(--sidebar-highlight-hover);
}

# ======================
# File: pages\supp_blog\p_0_radiancefields.md
# ======================

---
title: "Radiance Fields"
layout: "base/base_article.njk"
tags: "physics"
order : 0
---

ASD

# ======================
# File: plugins\markdown-containers.js
# ======================

// plugins/markdown-containers.js

import container from "markdown-it-container";

// 1. Define Custom Blocks
const customBlocks = [
  { name: "note", title: "Note" },
  { name: "tip", title: "Tip" },
  { name: "warning", title: "Warning" },
  { name: "important", title: "Important" },
  { name: "caution", title: "Caution" },
  { name: "math", title: "Definition" },
  { name: "concept", title: "Concept" },
  { name: "idea", title: "Core Idea" },
];

// 2. Configure Containers
export default function configureContainers(md) {
  customBlocks.forEach((block) => {
    md.use(container, block.name, {
      render: function (tokens, idx) {
        const token = tokens[idx];
        if (token.nesting === 1) {
          const rawInfo = token.info.trim().slice(block.name.length).trim();

          let displayTitle = "";

          if (!rawInfo) {
            displayTitle = block.title;
          } else if (rawInfo.startsWith("_")) {
            const customText = rawInfo.slice(1).trim();
            displayTitle = `${block.title}: ${customText}`;
          } else {
            displayTitle = rawInfo;
          }

          return `<div class="callout callout-${block.name}">
                    <p class="callout-title">${md.utils.escapeHtml(displayTitle)}</p>\n`;
        } else {
          return `</div>\n`;
        }
      },
    });
  });
}


# ======================
# File: scripts\auth_hash.js
# ======================

        const CORRECT_HASH = "b2581e1bdbe7c3fdcbe6814978fcce82fc76a8ed3ccc2ab69717f91be294df98";
        const XOR_CONSTANT_B64 = "HgECVgICU2kIWVYAHUteWQcEHUpfRwlc";
        document.getElementById("submitButton").addEventListener("click", checkPassword);
        document.getElementById("passwordInput").addEventListener("keyup", (event) => {
            if (event.key === "Enter") {
                checkPassword();
            }
        });

        async function checkPassword() {
            const input = document.getElementById("passwordInput").value;
            const messageEl = document.getElementById("message");
            
            const inputHash = await hashString(input);
        
            if (inputHash === CORRECT_HASH) {
                messageEl.textContent = "";
        
                const XOR_CONSTANT = atob(XOR_CONSTANT_B64); 
        
                const salt = await hashString(input + input);
                const saltedInput = input + salt;
                const secondHash = await hashString(saltedInput);
        
                const key = secondHash.substring(0, XOR_CONSTANT.length);
                const path = xorStrings(key, XOR_CONSTANT);
        
                console.log("Access granted. Redirecting to:", path);
                window.location.href = path;
        
            } else {
                messageEl.textContent = "Access Denied";
            }
        }

        async function hashString(str) {
            const encoder = new TextEncoder();
            const data = encoder.encode(str);
            const hashBuffer = await crypto.subtle.digest('SHA-256', data);
            
            const hashArray = Array.from(new Uint8Array(hashBuffer));
            const hexHash = hashArray.map(b => b.toString(16).padStart(2, '0')).join('');
            return hexHash;
        }

        function xorStrings(s1, s2) {
            let result = '';
            const length = Math.min(s1.length, s2.length);

            for (let i = 0; i < length; i++) {
                result += String.fromCharCode(s1.charCodeAt(i) ^ s2.charCodeAt(i));
            }
            return result;
        }

# ======================
# File: scripts\auth_simple.js
# ======================

console.log(document.getElementById("submitButton"));
PATH_DIPL = "/sections/section_dipl/index.html"
PATH_COMPBIO = "/sections/section_compbio/index.html"

document.getElementById("submitButton").addEventListener("click", checkPassword);
document.getElementById("passwordInput").addEventListener("keyup", (event) => {
    if (event.key === "Enter") {
        checkPassword();
    }
});

async function checkPassword() {
    const input = document.getElementById("passwordInput").value;
    const messageEl = document.getElementById("message");
    
    messageEl.textContent = "";

    if (input === "dipl") {
        window.location.href = PATH_DIPL;
    } else if (input === "compbio") {
        window.location.href = PATH_COMPBIO;
    } else {
        messageEl.textContent = "Access Denied";
    }
}


# ======================
# File: scripts\scrollarrow.js
# ======================

const arrow = document.getElementById('scroll-arrow');
const hideArrow = () => {
    if (arrow) {
        arrow.classList.add('fade-out');
        // Optional: Remove from DOM after fade finishes (0.5s)
        setTimeout(() => arrow.remove(), 500); 
    }
};

// Listen for interaction once, then clean up listeners
window.addEventListener('scroll', hideArrow, { once: true });
window.addEventListener('wheel', hideArrow, { once: true });
window.addEventListener('touchmove', hideArrow, { once: true });

# ======================
# File: scripts\inertia_simulator\inertia_simulator.js
# ======================

import * as THREE from "https://unpkg.com/three@0.162.0/build/three.module.js";
import { OrbitControls } from "https://unpkg.com/three@0.162.0/examples/jsm/controls/OrbitControls.js";
import GUI from "https://cdn.jsdelivr.net/npm/lil-gui@0.19/+esm";

// === Basic three.js setup ===
const canvas = document.getElementById("c");
const renderer = new THREE.WebGLRenderer({ canvas, antialias: true });
renderer.setPixelRatio(window.devicePixelRatio || 1);

const scene = new THREE.Scene();
scene.background = new THREE.Color(0x111111);

const camera = new THREE.PerspectiveCamera(45, 1, 0.1, 100);
camera.position.set(4, 3, 6);

const controls = new OrbitControls(camera, renderer.domElement);
controls.enableDamping = true;
controls.dampingFactor = 0.05;

function onResize() {
    const width = window.innerWidth;
    const height = window.innerHeight;
    renderer.setSize(width, height, false);
    camera.aspect = width / height;
    camera.updateProjectionMatrix();
}
window.addEventListener("resize", onResize);
onResize();

// === Lights ===
{
    const ambient = new THREE.AmbientLight(0xffffff, 0.5);
    scene.add(ambient);

    const dir = new THREE.DirectionalLight(0xffffff, 1);
    dir.position.set(5, 10, 7);
    scene.add(dir);
}

// === Coordinate axes helper ===
const axesHelper = new THREE.AxesHelper(2.5);
scene.add(axesHelper);

// === Rod geometry (rectangular bar) ===
// Treat the rod as a solid cuboid with length L along the local X-axis
const mass = 1.0;
const Lx = 3.0;   // length
const Ly = 0.3;   // thickness
const Lz = 0.3;   // thickness

const rodGeom = new THREE.BoxGeometry(Lx, Ly, Lz);
const rodMat = new THREE.MeshStandardMaterial({
    color: 0x6699ff,
    metalness: 0.2,
    roughness: 0.4
});
const rod = new THREE.Mesh(rodGeom, rodMat);
scene.add(rod);

// Start with rod at some skewed orientation
rod.rotation.set(0.6, 0.4, 0.3);

// === Inertia tensor in body frame (about center of mass) ===
// For a solid cuboid of mass m and edge lengths Lx, Ly, Lz:
// Ixx = (1/12) m (Ly^2 + Lz^2), etc.
const Ixx = (mass / 12) * (Ly * Ly + Lz * Lz);
const Iyy = (mass / 12) * (Lx * Lx + Lz * Lz);
const Izz = (mass / 12) * (Lx * Lx + Ly * Ly);

const Ibody = new THREE.Matrix3().set(
    Ixx, 0,   0,
    0,   Iyy, 0,
    0,   0,   Izz
);

// === Physics state ===
let angularVelocity = new THREE.Vector3(0, 0, 0); // world coordinates
const torqueWorld = new THREE.Vector3(0, 1, 0);   // will be controlled via GUI

// === Visual arrows for torque and response axis ===
const torqueColor = 0x00ffff;  // cyan
const alphaColor  = 0xff00ff;  // magenta

const torqueArrow = new THREE.ArrowHelper(
    torqueWorld.clone().normalize(),
    new THREE.Vector3(0, 0, 0),
    2.0,
    torqueColor,
    0.2,
    0.1
);
scene.add(torqueArrow);

const alphaArrow = new THREE.ArrowHelper(
    new THREE.Vector3(1, 0, 0),
    new THREE.Vector3(0, 0, 0),
    2.0,
    alphaColor,
    0.2,
    0.1
);
scene.add(alphaArrow);

// === GUI for user-specified torque axis ===
const params = {
    axisX: 0,
    axisY: 1,
    axisZ: 0,
    magnitude: 1.0,
    resetRod: () => {
    rod.rotation.set(0.6, 0.4, 0.3);
    rod.updateMatrixWorld(true);
    angularVelocity.set(0, 0, 0);
    },
    pause: false
};

const gui = new GUI();
const folderAxis = gui.addFolder("Torque axis (world)");
folderAxis.add(params, "axisX", -1, 1, 0.01).name("x").onChange(updateTorqueVector);
folderAxis.add(params, "axisY", -1, 1, 0.01).name("y").onChange(updateTorqueVector);
folderAxis.add(params, "axisZ", -1, 1, 0.01).name("z").onChange(updateTorqueVector);
folderAxis.open();

gui.add(params, "magnitude", 0, 5, 0.01).name("magnitude").onChange(updateTorqueVector);
gui.add(params, "pause").name("pause simulation");
gui.add(params, "resetRod").name("reset rod");

function updateTorqueVector() {
    const axis = new THREE.Vector3(params.axisX, params.axisY, params.axisZ);
    if (axis.lengthSq() < 1e-6) {
    // Avoid zero vector; default to Y axis
    axis.set(0, 1, 0);
    }
    axis.normalize();
    torqueWorld.copy(axis).multiplyScalar(params.magnitude);

    torqueArrow.setDirection(axis);
    torqueArrow.setLength(1.0 + 0.5 * params.magnitude);
}

updateTorqueVector();

// === Utility: compute inertia tensor in world frame ===
const tmpMat3A = new THREE.Matrix3();
const tmpMat3B = new THREE.Matrix3();

function computeWorldInertia() {
    // Extract rotation part of rod's world matrix
    rod.updateMatrixWorld(true);
    const R = tmpMat3A.setFromMatrix4(rod.matrixWorld); // rotation only
    const Rt = tmpMat3B.copy(R).transpose();

    // I_world = R * I_body * R^T
    const Iworld = new THREE.Matrix3();
    Iworld.multiplyMatrices(R, Ibody);
    Iworld.multiply(Rt);
    return Iworld;
}

// === Info panel ===
const infoDiv = document.getElementById("info");

function fmt(x) {
    return x.toFixed(3).padStart(8, " ");
}

function updateInfo(Iworld, tau, alpha) {
    const e = Iworld.elements;
    // Convert column-major array to row-wise display
    const row1 = [e[0], e[1], e[2]];
    const row2 = [e[3], e[4], e[5]];
    const row3 = [e[6], e[7], e[8]];

    const t = tau;
    const a = alpha;

    infoDiv.textContent =
    "World-frame inertia tensor I (about center of mass)\n" +
    "Rows/cols in world (x, y, z)\n\n" +
    "I = [ " + fmt(row1[0]) + " " + fmt(row1[1]) + " " + fmt(row1[2]) + " ]\n" +
    "    [ " + fmt(row2[0]) + " " + fmt(row2[1]) + " " + fmt(row2[2]) + " ]\n" +
    "    [ " + fmt(row3[0]) + " " + fmt(row3[1]) + " " + fmt(row3[2]) + " ]\n\n" +
    "Torque τ (world): ("  + fmt(t.x) + ", " + fmt(t.y) + ", " + fmt(t.z) + " )\n" +
    "Instantaneous α = I⁻¹ τ (world):\n" +
    "             ("  + fmt(a.x) + ", " + fmt(a.y) + ", " + fmt(a.z) + " )\n\n" +
    "Cyan arrow: torque axis (τ)\n" +
    "Magenta arrow: response axis (α, angular acceleration)\n" +
    "Off-diagonal terms of I are what make α deviate from τ.";
}

// === Main animation loop ===
let lastTime = performance.now();

function animate(now) {
    requestAnimationFrame(animate);

    const dt = Math.min(0.033, (now - lastTime) / 1000); // cap dt for stability
    lastTime = now;

    // Compute inertia tensor in world frame
    const Iworld = computeWorldInertia();

    // Compute angular acceleration in world frame: α = I_world⁻¹ * τ
    const IworldInv = Iworld.clone();
    IworldInv.invert(); // assuming it's invertible (true for our rigid body)
    const alphaWorld = torqueWorld.clone().applyMatrix3(IworldInv);

    if (!params.pause) {
    // Integrate angular velocity: ω_{t+dt} = ω_t + α dt
    angularVelocity.addScaledVector(alphaWorld, dt);

    // Apply rotation to rod using ω in world coordinates
    const wMag = angularVelocity.length();
    if (wMag > 1e-6) {
        const axis = angularVelocity.clone().normalize();
        const angle = wMag * dt;
        const dq = new THREE.Quaternion().setFromAxisAngle(axis, angle);

        // Apply world-space rotation: q_new = dq * q
        rod.quaternion.premultiply(dq);
    }
    }

    // Update alpha arrow visualization
    const alphaAxis = alphaWorld.clone();
    if (alphaAxis.lengthSq() < 1e-8) {
    alphaAxis.set(1, 0, 0);
    }
    const alphaLen = Math.min(2.0, 0.5 + alphaWorld.length());
    alphaArrow.setDirection(alphaAxis.normalize());
    alphaArrow.setLength(alphaLen);

    // Update info panel
    updateInfo(Iworld, torqueWorld, alphaWorld);

    controls.update();
    renderer.render(scene, camera);
}

animate(performance.now());


# ======================
# File: unreleased\pointers_references.md
# ======================

*aruisdante* answer to question poised at reddit thread:

*https://www.reddit.com/r/cpp/comments/18186it/i_am_absolutely_confused_on_the_topic_of/*

# references vs pointers

## Pointer’s Semantics
A pointer is a distinct value type. It has a defined size on a given platform, which can be obtained by sizeof(void*). On a platform with a true 64 bit address space, this will return 8. All pointers to any object type will have this same size: sizeof(T*) == sizeof(U*). The value of a pointer is an actual, real memory location. You can introspect this directly; if you print a pointer without dereferencing it, the value printed is the memory address value the pointer holds.

Basically, pointers are “true” value types in an of themselves and can exist independently of having an object to point to. This is why you can have a null pointer. And since they are true value types, they have their own definition of the following operations, independent of the type they point to:

To compare a pointer means to compare the memory address, not the content at that memory address.

To copy/assign a pointer means to copy/assign the memory address, not the content at that memory address.

Taking the address of a pointer will produce the address of the pointer object itself, not the pointed to content.

“Dereferencing” a pointer produces a reference to the pointed to content. I know, it’s confusing. It made more sense in C when there wasn’t a distinct thing called a reference, so pointers were said to have “reference semantics” and thus “dereferencing” them produced the actual content.

When we say a type is “pointer like,” we mean it models the properties above. This is why the smart pointers behave the way they do. It’s also why std::span is so confusing. std::span looks like a container, so you expect it to behave the way a reference to a container would: comparing it would compare the elements in the span. But you can reassign spans, and that assignment doesn’t reassign the underlying elements, it just reassigns the underlying pointer to point to new elements. So to be consistent, the standard chose the lesser of two evils and made span model a pointer, and thus has pointer semantics all around. When you compare a span, you’re asking “do these spans provide a view onto literally the same content in memory,” just like comparing a pointer. In C++20 they formalized this “container reference API but pointer semantics” notion with the view concept.

## Reference’s Semantics
A reference is not actually a distinct type, just like that the second quote states: there is no actual object that “represents” a reference. You cannot take the address of the reference itself. If you do sizeof(T&), you will get the same answer as sizeof(T). So most accurately, a reference is an alias for the actual instance of an object.

Because references are not true value types in and of themselves, but instead are aliases for the instance, they have the following semantics:

They must be “bound” to an instance at initialization. A reference can never be “null.”

They cannot be re-assigned. Assigning to a reference after it has been “bound” to an instance is equivalent to assigning to the bound instance, not the reference.

Comparing references is equivalent to comparing the referenced instances.

Taking the address of a reference gives the address of the bound instance.

## Implementations
As discussed above, a pointer is an explicitly defined value type. They take up memory on the stack/heap.

A reference, on the other hand, is not an explicitly defined value type. Because a reference isn’t a “real” value type, but instead defined as an alias to the bound instance, the compiler is free to implement references however it wants, as long as it maintains the semantics.

This means a compiler is free to optimize out a reference completely. The compiler cannot inherently do this with a pointer: a pointer has to actually exist because you can take its address, reassign it, etc. However, the compiler is just as free to implement a reference as a pointer (in the machine sense, see below), and often will do so if the reference passes between functions that cannot be inlined.

Why is this so confusing?
Two reasons:

### Conflating Pointer with Indirect Referencing
When people talk about pointers, they often use them interchangeably with the machine’s notion of a memory address and indirect referencing, because that’s what a (raw) pointer models. But these are actually two distinct things. Pointer the C/C++ type is an abstract notion of a memory address that can be used for indirect referencing. It has meaning independent from how a particular platform implements indirect referencing; there is no “pointer” type in assembly, just instructions that perform operations indirected through a memory address. So when folks say “a reference and a pointer are the same thing” they are thinking in terms of this generated machine assembly for indirect referencing, and how the generated assembly might store the addresses used for it.

### The “Reference Semantics” Colloquialism
There is a common colloquialism to refer both pointers and references as having “reference semantics.” What people mean when they say this is that a given variable name is indirectly representing an instance, without being a value type of that instance. It allows manipulation of the instance through the variable name, and passing the instance between functions without having to copy it. However this colloquialism is a misnomer. A pointer does not, in fact, have reference semantics. It is a value type. It is that value type which models reference semantics, through the * and -> operators.

You’ll often hear this pointed out by folks when they ask “does Python/Java have reference semantics or value semantics?” It’s a trick question: the correct answer is “they have value semantics, but the only value type you can directly assign to a name is a pointer.” That said, the lack of a true reference, or an assignable value type other than a pointer, is key to understanding why Java and Python behave the way they do.

## Brining it home
So back to the original quotes. I hope after reading this you can see why neither quote is actually wrong, nor do they contradict each other. They’re just speaking from different perspectives. Bjarne is focused on what he was trying to accomplish when he added references to C++, which C lacked. He wanted something that had the indirect reference properties of a pointer, but with better properties for the common usage of pointers in most C programs, which was to alias a single object: references have a nicer syntax for this case (no need for * and ->), and lack the possibility of edge cases like null pointer dereferences or accidental reassignment of the pointer rather than the pointed to object if you forgot to dereference the pointer first. This latter problem was particularly important to avoid in a language that allows implicit conversion between integral types and pointers: writing some_int_ptr = some_int when you meant to write *some_int_ptr = some_int, or some_int = some_int_ptr when you meant some_int = *some_int_ptr, was a shockingly common source of bugs in C. But he didn’t want to have to invent entirely new ways of doing things: remember the original C++ was just a code generator for C. So he had the clever idea to create “a pointer, but with restrictions.”

The second quote, on the other hand, is more accurately describing what the language actually defines a reference as. Bjarne’s mental model when he was designing references may have been “a pointer with restrictions,” but what he codified was just the indirect reference properties, not actually anything to do with the actual type pointer.

# ======================
# File: _data\navigation.json
# ======================

[
  {
    "label": "Thesis Home",
    "url": "/sections/dipl/",
    "id": "dipl",
    "chatUrl": "/chats/dipl_chats/"
  },
  {
    "label": "Supplementary Blog",
    "url": "/sections/supp_blog/",
    "id": "supp",
    "chatUrl": "/chats/supp_chats/"
  },
  {
    "label": "Mathematics",
    "url": "/sections/mathematics/",
    "id": "mathematics",
    "chatUrl": "/chats/chats_mathematics/"
  },
  {
    "label": "3D Gaussian Splatting",
    "url": "/sections/3dgs/",
    "id": "3dgs",
    "chatUrl": "/chats/chats_3dgs/"
  },
  {
    "label": "Data Structures",
    "url": "/sections/ds/",
    "id": "ds",
    "chatUrl": "/chats/ds_chats/"
  },
  {
    "label": "Computational Biology",
    "url": "/sections/compbio/",
    "id": "compbio",
    "chatUrl": "/chats/compbio_chats/"
  },
  {
    "label": "DL: Computer Vision",
    "url": "/sections/dlcv/",
    "id": "dlcv",
    "chatUrl": "/chats/dlcv_chats/"
  }
]